{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Mean Discrepancy(MMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAM matrix\n",
    "By given a set of vectors $a_k$, Gram Matrix is simply calculated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "G=A^TA=\n",
    "\\begin{bmatrix}\n",
    "a_1\\\\\n",
    "a_2\\\\\n",
    "\\vdots\\\\\n",
    "a_n\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "a_1 & a_2 & \\cdots & a_n\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "a_1^Ta_1 & a_1^Ta_2 & \\cdots & a_1^Ta_n \\\\\n",
    "a_2^Ta_1 & a_2^Ta_2 & \\cdots & a_2^Ta_n \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "a_n^Ta_1 & a_n^Ta_2 & \\cdots & a_n^Ta_n \n",
    "\\end{bmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is a matrix whose columns are the vectors $a_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hermitian matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_{ij}=\\overline{a_{ji}}$\n",
    "\n",
    "$A = \\overline{A^T}$\n",
    "\n",
    "Hermitian matrix is the complex extension of a real symmetric matrix (Conjugate transpose).\n",
    "\n",
    "For complex matrix, $G=A^HA$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernal function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K(\\pmb x_i, \\pmb x_j) = \\Phi(\\pmb x_i)\\Phi(\\pmb x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guassian_kernel(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    \"\"\"计算Gram核矩阵\n",
    "    source: sample_size_1 * feature_size 的数据\n",
    "    target: sample_size_2 * feature_size 的数据\n",
    "    kernel_mul: 这个概念不太清楚，感觉也是为了计算每个核的bandwith\n",
    "    kernel_num: 表示的是多核的数量\n",
    "    fix_sigma: 表示是否使用固定的标准差\n",
    "        return: (sample_size_1 + sample_size_2) * (sample_size_1 + sample_size_2)的\n",
    "                        矩阵，表达形式:\n",
    "                        [   K_ss K_st\n",
    "                            K_ts K_tt ]\n",
    "    \"\"\"\n",
    "    n_samples = int(source.size()[0])+int(target.size()[0])\n",
    "    total = torch.cat([source, target], dim=0) # 合并在一起\n",
    "\n",
    "    total0 = total.unsqueeze(0).expand(int(total.size(0)), \\\n",
    "                                       int(total.size(0)), \\\n",
    "                                       int(total.size(1)))\n",
    "    total1 = total.unsqueeze(1).expand(int(total.size(0)), \\\n",
    "                                       int(total.size(0)), \\\n",
    "                                       int(total.size(1)))\n",
    "    L2_distance = ((total0-total1)**2).sum(2) # 计算高斯核中的|x-y|\n",
    "\n",
    "    # 计算多核中每个核的bandwidth\n",
    "    if fix_sigma:\n",
    "        bandwidth = fix_sigma\n",
    "    else:\n",
    "        bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "    bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "    bandwidth_list = [bandwidth * (kernel_mul**i) for i in range(kernel_num)]\n",
    "\n",
    "    # 高斯核的公式，exp(-|x-y|/bandwith)\n",
    "    kernel_val = [torch.exp(-L2_distance / bandwidth_temp) for \\\n",
    "                  bandwidth_temp in bandwidth_list]\n",
    "\n",
    "    return sum(kernel_val) # 将多个核合并在一起\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd(source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "    n = int(source.size()[0])\n",
    "    m = int(target.size()[0])\n",
    "\n",
    "    kernels = guassian_kernel(source, target, kernel_mul=kernel_mul, kernel_num=kernel_num, fix_sigma=fix_sigma)\n",
    "    XX = kernels[:n, :n] \n",
    "    YY = kernels[n:, n:]\n",
    "    XY = kernels[:n, n:]\n",
    "    YX = kernels[n:, :n]\n",
    "\n",
    "    XX = torch.div(XX, n * n).sum(dim=1).view(1,-1)  \n",
    "    XY = torch.div(XY, -n * m).sum(dim=1).view(1,-1)\n",
    "\n",
    "    YX = torch.div(YX, -m * n).sum(dim=1).view(1,-1)\n",
    "    YY = torch.div(YY, m * m).sum(dim=1).view(1,-1)\n",
    "    \t\n",
    "    loss = (XX + XY).sum() + (YX + YY).sum()\n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = torch.tensor(np.random.normal(loc=0,scale=10,size=(100,50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = torch.tensor(np.random.normal(loc=10,scale=10,size=(90,50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0417, dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmd(data_1,data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
